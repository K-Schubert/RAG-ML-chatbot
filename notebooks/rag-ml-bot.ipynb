{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50ac428-1b13-45e5-ba47-fb77486714ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranschubert/Desktop/RAG-ML-chatbot/venv_rag/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import pinecone\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3a03ce-bf21-4254-a74f-630192e9dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e849f43",
   "metadata": {},
   "source": [
    "# Initialize chatbot\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbba51-ae23-4b8b-be50-4ed2d9275efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chatbot\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo-0301',\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779626f-8fb7-4fb9-819d-7dc2e2da3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand Parameter Efficient Fine-Tuning.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a203cb-0d64-4322-a945-fcba434aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6966ef-adf2-43f0-9b6a-a7341e5b35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you describe several PEFT methods?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2b0ba",
   "metadata": {},
   "source": [
    "# New Prompt Templates for ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup first system message\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        'You are a helpful assistant. You keep responses to no more than '\n",
    "        '100 characters long (including whitespace), and sign off every '\n",
    "        'message with a random name.'\n",
    "    )),\n",
    "    HumanMessage(content=\"Hi AI, how are you? Can you explain Parameter Efficient Fine-Tuning (PEFT)?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "# Too long\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input} Can you keep the response to no more than 100 characters '+\n",
    "    '(including whitespace), and sign off with a random name.'\n",
    ")\n",
    "\n",
    "# create the human message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_template])\n",
    "\n",
    "# format with some input\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    input=\"Hi AI, how are you? Can you explain Parameter Efficient Fine-Tuning (PEFT)?\"\n",
    ")\n",
    "\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35945",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d02b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        'You are a helpful assistant. You keep responses to no more than '\n",
    "        '100 characters long (including whitespace), and sign off every '\n",
    "        'message with a random name.'\n",
    "    )),\n",
    "    chat_prompt.format_prompt(\n",
    "        input=\"Hi AI, how are you? Can you explain Parameter Efficient Fine-Tuning (PEFT)?\"\n",
    "    ).to_messages()[0]\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f302adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    'You are a helpful assistant. You keep responses to no more than '\n",
    "    '{character_limit} characters long (including whitespace), and sign '\n",
    "    'off every message with \"- {sign_off}'\n",
    ")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "ai_template = AIMessagePromptTemplate.from_template(\"{response} - {sign_off}\")\n",
    "\n",
    "# create the list of messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_template,\n",
    "    human_template,\n",
    "    ai_template\n",
    "])\n",
    "# format with required inputs\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    character_limit=\"100\", sign_off=\"Your trustworthy AI\",\n",
    "    input=\"Can you explain Parameter Efficient Fine-Tuning (PEFT)?\",\n",
    "    response=\"PEFT is a method to fine-tune a pre-trained model with fewer parameters.\"\n",
    ")\n",
    "\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_prompt_value.to_messages()\n",
    "\n",
    "messages.append(\n",
    "    HumanMessage(content=\"How many parameters?\")\n",
    ")\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34249291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a faster way of building the prompt via a PromptTemplate\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input} Answer in less than {character_limit} characters (including whitespace).'\n",
    ")\n",
    "\n",
    "# create the human message\n",
    "human_prompt = ChatPromptTemplate.from_messages([human_template])\n",
    "\n",
    "# format with some input\n",
    "human_prompt_value = human_prompt.format_prompt(\n",
    "    input=\"When should I use PEFT?\",\n",
    "    character_limit=\"100\"\n",
    ")\n",
    "\n",
    "human_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last message\n",
    "messages.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(human_prompt_value.to_messages())\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8201e-6fec-4609-9ecf-e94d98f9ca1f",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993a6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embedding model \"text-embedding-ada-002\" from openAI to create vector embeddings\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\",\n",
    "                               disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc0bf0d-ec0f-4d92-bf6b-c64ea4340740",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6341dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranschubert/Desktop/RAG-ML-chatbot/venv_rag/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_field = \"text\"\n",
    "index_name = \"rag-ml\"\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce0296e-4b93-4d78-9bf9-da0d92963bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Although ICL provides a viable alternative to full fine-tuning,\\nit operates at inference time, and it neither allows learning nor\\nupdating any parameters, which may prevent capturing more fine-\\ngrained information about the task. It can result in a potential loss\\nof effectiveness. In this context, Parameter-Efficient Fine-Tuning\\n(PEFT) techniques have emerged as promising solutions to ren-\\nder the fine-tuning cost at the lowest while allowing the model\\nto learn task-specific parameters. Prior works [ 11,64,65] in code\\nintelligence have demonstrated the capability of PEFT techniques,\\nand often shown their superiority over full fine-tuning across a\\nwide range of tasks. However, these studies focus on small lan-\\nguage models ( <0.25B parameters) such as CodeBERT [ 15] and\\nCodeT5 [ 66] and overlooked the applicability of PEFT techniques toarXiv:2308.10462v1  [cs.SE]  21 Aug 2023\\nConference’17, July 2017, Washington, DC, USA Weyssow et al.\\nLLMs (≥1B parameters), leaving an important research gap. Given\\nthe growing ubiquity of LLMs, we believe addressing this gap is\\nof paramount importance in advancing the field of code intelli-\\ngence and harnessing the full potential of LLMs. Furthermore, we\\nidentify an additional research opportunity in exploring the usage\\nof PEFT techniques under limited resource scenarios, aiming to\\ndemonstrate the democratization of LLMs tuning through PEFT.\\nAddressing these gaps will not only show how PEFT techniques\\ncan enhance the effectiveness of LLMs but also how they allow to\\nbroaden the accessibility and utility of LLMs in scarce computation\\nsettings.\\nIn this paper, we delve into the realm of PEFT techniques for', metadata={'source': 'http://arxiv.org/pdf/2308.10462v1', 'title': 'Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models'}),\n",
       " Document(page_content='performance improvements and may be integrated into closed -\\nsource models like GPT-3.5 and GPT-4 [ 9]. The open-source\\nStanford Alpaca approach fine-tunes all parameters of LLMs\\nin an end-to-end manner.\\nParameter-Efficient Fine-Tuning: Parameter-Efficient\\nFine-Tuning (PEFT) [ 2] aims to optimize the fine-tuning\\nprocess by efficiently utilizing the available computing\\nresources and reducing the number of parameters that need\\nto be updated. This approach becomes particularly relevant\\nwhen working with limited labeled data for a specific\\ntask. This approach not only saves computational time\\nand resources but also enables the deployment of large\\nlanguage models more accessible and practical for a wide\\nrange of applications. Various PEFT techniques include\\nPrefix Tuning [ 7], Low-Rank adaptation (LoRA) [ 5], and\\nthe insertion of adapter layers in pre-trained large langua ge\\nmodels. Prefix Tuning [ 7] appends a collection of prefixes to\\nautoregressive language models, or alternatively, incorp orates\\nprefixes for both encoder and decoder components, similar\\nmethods proposed in [ 6]. LoRA [ 5] introduces trainable rank\\ndecomposition matrices into each layer. Adapters involve\\ninserting lightweight modules into each layer of pre-train ed\\nmodels, which only updates the adapters and has been\\nextended across numerous domains.\\nIII. M ETHOD\\nIn this section, we will describe our method to collect\\ndatasets, choose pretrained language model and how we apply\\nparameter-efficient fine-tuning.\\nA. Data Collection\\nAs creating an instruction tuning dataset with many tasks\\nfrom scratch would be resource-intensive, we transform\\nexisting datasets from the research community into an\\ninstructional format and translate them into Vietnamese\\nInstruction dataset example:', metadata={'source': 'http://arxiv.org/pdf/2309.04646v1', 'title': 'Efficient Finetuning Large Language Models For Vietnamese Chatbot'}),\n",
       " Document(page_content='interesting opportunity for the mixture-of-expert type of modeling without parallelization overhead.\\nChen et al. (2023) experiment with different design spaces (essentially a hyperparameter search)\\nfor PEFT. They suggest four phases: 1) grouping layers into different sets; 2) adding trainable\\nparameters towards each group; 3) deciding which group should be trained; 4) assigning groups with\\ndifferent training strategies. Their finding is that different architectures have different best settings.\\nWe have chosen (IA)3and LORA as our PEFT components because they offer an optimal balance\\nbetween performance and parameter efficiency (Mahabadi et al., 2021; Liu et al., 2022).\\nSeveral studies have explored PEFT in the context of MoE or in a similar fashion, albeit with\\ncertain distinctions. For instance, Wang et al. (2022) focused on single-task fine-tuning employing a\\nmixture of adapters for BERT basewith 110M parameters (Devlin et al., 2019) and RoBERTa large\\nwith 355M parameters (Liu et al., 2019), incorporating random routing, and adopting a few-shot\\nevaluation. In divergence from this, our work centers on instruction-tuning with multiple tasks\\npresent during fine-tuning. We underscore the efficacy of this approach by rigorously testing up\\nto 11B parameter text-to-text model Raffel et al. (2020), implementing token routing, and strictly\\nemphasizing evaluation on a set of unseen (held-out) tasks to underscore the potential of instruction\\ntuning. In another work, Ponti et al. (2022) introduced Polytropon, which involves learning adapters\\n(termed as ’skills’) specific to each task and employing a task-skills binary matrix to determine', metadata={'source': 'http://arxiv.org/pdf/2309.05444v1', 'title': 'Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I'd like to understand Parameter Efficient Fine-Tuning. Can you describe several PEFT methods?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fafc05-045e-44d8-9c06-a58229f706ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(query: str):\n",
    "\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "    metadata = [{\"title\": x.metadata[\"title\"], \n",
    "                \"source\": x.metadata[\"source\"]} for x in result]\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd84df1-ccc8-4347-90da-8b41a5a62b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    \n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    \n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abfccf-fe7f-48a8-bc22-0cf2d9c13471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749b2e9-b2fa-46fe-a81e-9529292ac6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\")\n",
    "]\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)\n",
    "print(get_metadata(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9733153-d14f-4711-9a5f-1687202670f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt without RAG\n",
    "prompt = HumanMessage(\n",
    "    content=\"What can you tell me about LoRA training?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b629129-26be-4755-a87e-016838503180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with RAG\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"What can you tell me about LoRA training?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3290f-65b6-4297-91ca-43125f585df6",
   "metadata": {},
   "source": [
    "# Insights for Naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a62cb-606c-42d1-b716-26ad15e73988",
   "metadata": {},
   "source": [
    "- \"Naive RAG\": simplest way of implementing RAG -> assumes question in every query (sometimes bot doesn't need to access KB to answer)\n",
    "- Ability to cite sources\n",
    "- Faster than using agents\n",
    "- Can filter number of tokens sent to LLM (with similarity threshold)\n",
    "- Token usage/cost is higher due to extended context\n",
    "- Too much context will degrade prompt\n",
    "\n",
    "Next steps\n",
    "- --> Agent RAG\n",
    "- --> Guardrails RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ce122-5f06-4fa0-887a-80ea9a8890ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7dc731d-04f3-4453-b406-32806c8ed630",
   "metadata": {},
   "source": [
    "- RAG Agent: Wrapper around LLM -> can have thoughts, internal dialogue (can reply immediately if no external knowledge is required, or access KB through )\n",
    "- Agent has access to external tools (eg. retrieval tool)\n",
    "- Agent decides when it has to use a specific tool\n",
    "- Slower (in langchain 3x LLM generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c63fe-4274-4fc4-a3be-b10239c160fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd3d1cee-59e0-4570-bca9-f706c0b6bd58",
   "metadata": {},
   "source": [
    "- Guardrails: in the middle of Naive RAG and RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d19470-60df-4878-a396-ff41fff29d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b0b95a",
   "metadata": {},
   "source": [
    "# Generative Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c874b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(fetch_k=20, k=5, return_source_documents=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2327b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter-Efficient Fine-Tuning (PEFT) aims to optimize the fine-tuning process by efficiently utilizing computing resources and reducing the number of parameters that need to be updated. Here are several PEFT methods:\n",
      "\n",
      "1. Prefix Tuning: This method appends a collection of prefixes to autoregressive language models. It can also incorporate prefixes for both encoder and decoder components. Prefix Tuning has been proposed as a way to improve performance while reducing the number of parameters that need to be updated.\n",
      "\n",
      "2. Low-Rank adaptation (LoRA): LoRA introduces trainable rank decomposition matrices into each layer of the pre-trained language model. By using low-rank matrices, LoRA reduces the number of parameters that need to be updated during fine-tuning.\n",
      "\n",
      "3. Adapters: Adapters involve inserting lightweight modules into each layer of pre-trained models. These modules, called adapters, are the only parameters that are updated during fine-tuning. Adapters have been extended across numerous domains and have shown promise in reducing the number of parameters that need to be fine-tuned.\n",
      "\n",
      "These methods, among others, offer ways to make fine-tuning more parameter-efficient, saving computational time and resources while still allowing for effective fine-tuning of large language models.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb7f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(fetch_k=20, k=1, return_source_documents=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d05445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to understand Parameter Efficient Fine-Tuning. Can you describe several PEFT methods?\n",
      "Several PEFT methods include Prefix Tuning, Low-Rank adaptation (LoRA), and the insertion of adapter layers in pre-trained large language models. Prefix Tuning appends a collection of prefixes to autoregressive language models, LoRA introduces trainable rank decomposition matrices into each layer, and adapters involve inserting lightweight modules into each layer of pre-trained models. Chen et al. (2023) experiment with different design spaces for PEFT, while Wang et al. (2022) focus on single-task fine-tuning using a mixture of adapters. Ponti et al. (2022) introduce Polytropon, which involves learning adapters specific to each task. These methods aim to optimize the fine-tuning process by efficiently utilizing computing resources and reducing the number of parameters that need to be updated. They allow for the deployment of large language models in a more accessible and practical manner. However, there is ongoing research on applying PEFT techniques to larger language models and exploring their usage under limited resource scenarios. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = qa_with_sources(query)\n",
    "\n",
    "print(res[\"question\"])\n",
    "print(res[\"answer\"])\n",
    "print(res[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378d2c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"sources\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1667bd-30d6-4e7a-a4fc-807c2c9bbce3",
   "metadata": {},
   "source": [
    "# Conversational Agent with tool (RetrievalQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf4297b-9956-49c8-8dca-cc13a222cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history', # refers to conversational agent component\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # place (\"stuff\") retrieved items item RetrievalQA (no summarization)\n",
    "    retriever=vectorstore.as_retriever(fetch_k=15, k=5, return_source_documents=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282c98e-5330-4710-9775-d5855d5ecf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi, how are you?\"\n",
    "\n",
    "# *** Only RetrievalQA - NOT Conversational Agent ***\n",
    "print(qa.run(query))\n",
    "\n",
    "query = \"can you tell me some facts Parameter Efficient Fine-Tuning?\"\n",
    "\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b06a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "# Add retrievalQA tool to agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d638afc-54ac-44e1-9180-84fac22d435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e8d297-bb48-4a47-8bd2-16c9a769d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Hi, how are you?', 'chat_history': [], 'output': \"I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"Parameter Efficient Fine-Tuning\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mParameter-efficient fine-tuning is a technique in natural language processing that aims to optimize the fine-tuning process by updating only a small number of additional parameters while keeping most of the pre-trained parameters frozen. This approach is particularly useful when working with limited labeled data for a specific task, as it saves computational time and resources. It also makes the deployment of large language models more accessible and practical for a wide range of applications. Various techniques can be used for parameter-efficient fine-tuning, such as prefix tuning, low-rank adaptation, and the insertion of adapter layers in pre-trained models.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Parameter-efficient fine-tuning is a technique in natural language processing that optimizes the fine-tuning process by updating only a small number of additional parameters while keeping most of the pre-trained parameters frozen. It is useful when working with limited labeled data for a specific task, as it saves computational time and resources. Various techniques can be used for parameter-efficient fine-tuning, such as prefix tuning, low-rank adaptation, and the insertion of adapter layers in pre-trained models.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'can you tell me some facts Parameter Efficient Fine-Tuning?', 'chat_history': [HumanMessage(content='Hi, how are you?', additional_kwargs={}, example=False), AIMessage(content=\"I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\", additional_kwargs={}, example=False)], 'output': 'Parameter-efficient fine-tuning is a technique in natural language processing that optimizes the fine-tuning process by updating only a small number of additional parameters while keeping most of the pre-trained parameters frozen. It is useful when working with limited labeled data for a specific task, as it saves computational time and resources. Various techniques can be used for parameter-efficient fine-tuning, such as prefix tuning, low-rank adaptation, and the insertion of adapter layers in pre-trained models.'}\n"
     ]
    }
   ],
   "source": [
    "# With Conversational Agent\n",
    "query = \"Hi, how are you?\"\n",
    "\n",
    "print(agent(query))\n",
    "\n",
    "query = \"can you tell me some facts Parameter Efficient Fine-Tuning?\"\n",
    "\n",
    "print(agent(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569d709-f1fa-49b4-9480-7d6d0b3da0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent(\"what is 2 * 7?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3561c5-891d-4672-9f1d-3362163bd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent(\"can you tell me some facts Parameter Efficient Fine-Tuning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10895dc-9911-4ab1-976f-465a66e5c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent(\"can you summarize these facts in two short sentences\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d96ae-afd9-4812-a306-df2ebc8a28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "venv_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
